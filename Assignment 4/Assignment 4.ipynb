{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ASSIGNMENT 4 <br>\n",
    "    NATURAL LANGUAGE PROCESSING</h1>\n",
    "<table align=\"left\" border=\"1\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td valign=\"top\" width=\"285\">\n",
    "<p>Presented by: Legends Consulting</p>\n",
    "<table style=\"border-style:hidden; border-collapse:collapse;\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td valign=\"top\" width=\"285\">\n",
    "<p align=\"right\">Benjamin Corbett</p>\n",
    "<p align=\"right\">Ngoc Nguyen</p>\n",
    "<p align=\"right\">Nathan Saric</p>\n",
    "<p align=\"right\">Hui Min Seng</p>\n",
    "</td>\n",
    "<td valign=\"top\" width=\"285\">\n",
    "<p>20093481</p>\n",
    "<p>20188794</p>\n",
    "<p>20099897</p>\n",
    "<p>20316721</p>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<p>Presented to: Division Capital - TradeSimple</p>\n",
    "<p>Datasets used:</p>\n",
    "<ol>\n",
    "<li><a href=\"https://www.kaggle.com/datasets/percyzheng/sentiment-classification-selflabel-dataset\"> Financial News Sentiment Classification Dataset</a></p></li>\n",
    "<li><a href=\"https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news\"> Financial Headlines Sentiment Classification Dataset</a></p></li>\n",
    "<li><a href=\"https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis\"> Financial Sentences Sentiment Classification Dataset</a></p></li>\n",
    "<li><a href=\"https://www.kaggle.com/datasets/sidarcidiacono/news-sentiment-analysis-for-stock-data-by-company\"> Stock Headlines Sentiment Classification Dataset</a></p></li>\n",
    "<li><a href=\"https://www.kaggle.com/datasets/chenxidong/stock-tweet-sentiment-dataset\"> Stock Tweets Sentiment Classification Dataset</a></p></li>\n",
    "</ol>\n",
    "<p>Algorithm used: BlazingText Classification</p>\n",
    "</td>\n",
    "<td valign=\"top\" width=\"285\">\n",
    "<p>WEIGHTS</p>\n",
    "<p>BUSINESS - 60%</p>\n",
    "<p>TECHNICAL - 30%</p>\n",
    "<p>PROFESSIONALISM - 10%</p>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Company Description</h2>\n",
    "<p style=\"text-align: justify;\"> After successfully taking their fintech startup, TradeSimple, public, a group of Queen’s alumni, flush with cash, decided to start an investment fund focused on public equities called Division Capital. Having experience building the 2nd largest retail trading platform, the team felt they had a deep enough understanding of financial markets and their inefficiencies, that they could outperform the market. Specifically, the team noticed how impactful retail investors could be in moving markets and sought to take advantage of this. So, the team got started developing an investment strategy that it could pitch to investors. </p>\n",
    "\n",
    "<h4>Investment Strategy</h4>\n",
    "<p style=\"text-align: justify;\"> As Queen’s business students, the team was all taught the Efficient Market Hypothesis throughout various courses, and thus, were strong believers that financial markets were highly efficient, controlled by large financial institutions, and unaffected by retail investors. However, after many years of working with TradeSimple, the team began to doubt their previous teachings and their relevance in the financial markets. Specifically, the effect that retail investors were having was something that the team took notice of. Often, large groups of retail investors, usually users of TradeSimple and other trading platforms, would come together on online forums and discuss their various investments. If one stock idea was talked about enough and popular enough, these groups would exhibit enough buyer-power together that the stock’s price would change by a significant amount. The most famous example of this was GameStop. </p>\n",
    "\n",
    "<h4><i>Gamestop</i></h4>\n",
    "<p style=\"text-align: justify;\"> Referring to Figure 1, within a month, GameStop’s share price rose from $17/share to $225/share, a 12x increase that came after no significant change in the company’s core operations, financial situation, or outlook. The reason was solely due to retail investors, something the market had never seen before. Through a popular trading forum “WallStreetBets”, thousands of individual investors came together and decided that GameStop was unfairly being punished by institutional investors’ mass short selling. As a video game retailer focused on brick-and-mortar stores, the company really had no future, however, the WallStreetBets community saw this as an opportunity to make a statement. That statement was clearly made, retail investors were capable of moving markets. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"./figure-1.jpg\" style=\"width:550px;height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Definition</h2>\n",
    "<p style=\"text-align: justify;\"> After witnessing this and other similar cases, the Division Capital team believed that they could profit off these events of rapid price changes if they could monitor online discussion. They theorized a tool that could track online conversations, and if enough positive or negative buzz was generated around one stock, an algorithm would automatically execute a trade. The key here is speed, the tool must be able to identify trends and immediately trade on those signals before investors flocked to the market, the difference could be only a few seconds. Using the example above, the Division Capital trading algorithm would have detected mass amounts of positive signals surrounding the GameStop stock and would automatically purchase GME shares before retail investors even opened their trading app. </p>\n",
    "\n",
    "<p style=\"text-align: justify;\"> As business students, the team was confident that they could take advantage of the inefficiencies of stocks influenced by retail investors, however, they did not have the technical expertise to develop the tools required to pull off these trades. They were aware of some potential technologies such as machine learning that could be used in their algorithm, so they sought the help of a consultant, Legends Consulting, experienced in developing machine learning algorithms. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction to the Solution</h2>\n",
    "<p style=\"text-align: justify;\"> The Legends team would like to propose a machine learning tool that performs financial sentiment analysis using the <em> Amazon's BlazingText Classification</em> algorithm. BlazingText is an extremely powerful tool that can perform natural language processing classification on large scale datasets. Taking text as an input, NLP classification can train an algorithm to categorize and tag data. This is useful in sentiment analysis, document recognition, information retrieval, ranking and much more. The tool for Division Capital will come in the form of sentiment analysis, meaning, it will take in text and determine whether that piece of text is positive, negative, or neutral, this will be the output of the model. In addition, since the model can never be completely certain, the other output will be a probability, the higher the probability, the more likely the classification is correct. The input into the model will be financial discussion, news, and headlines and will come in text form. Various datasets sourced from Kaggle have been collected that will be used to train the model. This data has a piece of financial information and a corresponding label, positive, negative, or neutral. For example, the text “Apple CEO is fired.” would be in one column, with the corresponding label “Negative” in the other column. In a business sense, positive sentiment is an indication that a stock could perform well, or at least, there is positive news surrounding a stock.</p>\n",
    "\n",
    "<p style=\"text-align: justify;\"> Once the model is trained, using a web scraper, text data (discussions, headlines, other news) on a specific stock can be collected in real-time and fed into the <em> BlazingText Classifiation </em> algorithm that we have created. The tool will then label the data as positive, negative, or neutral. If enough positive sentiment surrounding a specific security is identified, this would indicate to Division Capital that a stock is favoured in the market, and thus, should be purchased. This can be in the form of a percentage. For example, if 75% of the news surrounding a specific stock is positive and less than 10% is negative, issue a buy. </p>\n",
    "\n",
    "<p style=\"text-align: justify;\"> Where this tool is valuable, is in the speed in which it performs this task. Usually, for sentiment to translate to price movements in the market it takes a few minutes, up to hours. This algorithm, along with corresponding API’s and tools, can perform this task in a matter of seconds. The other tools that will be required are a web scraper that collects real-time financial data, and an API that can automatically trade based on a set conditions of sentiment levels. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setup the Environment</h2>\n",
    "<p style=\"text-align: justify;\"> Before we can load any data, we first initialize and configure a new <em>Amazon Sagemaker Session</em>. Additionally, we download several packages and tools in order to easily manipulate the datasets, such that they are compatible with <em>Amazon Sagemaker's BlazingText Classification</em> algorithm. We will use <em>pip</em>, a package installer for Python, to install <em>unzip</em> and <em>nltk</em>. The former is a tool that decompresses files and folders, whereas the latter is a natural languange toolkit. Note that this toolkit allows us to tokenize strings which is the required input format for this particular algorithm. Furthermore, these packages only need to be installed once, otherwise, the console will notify that these requirements are already satisfied. Finally, we generate a new <em>Amazon S3 Bucket </em>to store the necessary input and output files later in the model construction stage. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.80.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Creating an Amazon Sagemaker Session and creating variables for the Amazon Sagemaker S3 Bucket\n",
    "session = sagemaker.Session() \n",
    "bucket = session.default_bucket()\n",
    "region = session.boto_session.region_name\n",
    "\n",
    "# The current Sagemaker Version is 2.80.0\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unzip\n",
      "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: unzip\n",
      "  Building wheel for unzip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1330 sha256=935bc748006f341ed02573c48793ad114da3facdfb2a21fd1c4a650cf8dd4d41\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/2d/0f/c6/601ac10b1192955e02480f1ed815f72ab9b5283720930604c5\n",
      "Successfully built unzip\n",
      "Installing collected packages: unzip\n",
      "Successfully installed unzip-1.0.0\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installing the unzip utility tool needed to decompress files and folders\n",
    "!pip install unzip\n",
    "\n",
    "# Installing the nltk toolkit needed to tokenize strings\n",
    "!pip install ntlk\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Describe the Data</h2>\n",
    "<p style=\"text-align: justify;\"> With our <em>Amazon</em> environment properly setup, we can now prepare the dataset. Note that we will be combining six datasets from various sources, included in the <em>financial_data</em> folder, to construct a large enough dataset. The final dataset contains 43,394 lines of text relating to various financial or stock related news, headlines, and tweets, and has two columns: the <em>label</em> and the <em>text</em>. </p>\n",
    "\n",
    "<p style=\"text-align: justify;\"> We start by importing the compressed file containing the partitioned data from our local files and we will unzip the folder from within the environment using the <em>unzip</em> utility tool. Note that we choose to compress and decompress the dataset to avoid locally transferring the individual datasets to the environment which is time-consuming. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping the financial_data folder \n",
    "!unzip -qq financial_data.zip\n",
    "\n",
    "# Reading in each invidual dataset as a .csv file\n",
    "news      = pd.read_csv('financial_data/financial-news.csv', encoding='latin-1')\n",
    "headlines = pd.read_csv('financial_data/financial-headlines.csv', encoding='latin-1', names=['sentiment', 'headline'])\n",
    "sentences = pd.read_csv('financial_data/financial-sentences.csv', encoding='latin-1')\n",
    "djia      = pd.read_csv('financial_data/stock-djia-headlines.csv', encoding='latin-1')\n",
    "nasdaq    = pd.read_csv('financial_data/stock-nasdaq-headlines.csv', encoding='latin-1')\n",
    "tweets    = pd.read_csv('financial_data/stock-tweets.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------Dataset 1-----------------------------------\n",
      "text         object\n",
      "sentiment     int64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0     Global COVID-19 death toll exceeds 4 million.           0\n",
      "1  reports 67,208 new COVID-19 cases, 2,330 deaths.           0\n",
      "2  China reports 23 new COVID-19 cases versus 19 ...          0\n",
      "-----------------------------------Dataset 2-----------------------------------\n",
      "sentiment    object\n",
      "headline     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "  sentiment                                           headline\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "-----------------------------------Dataset 3-----------------------------------\n",
      "Sentence     object\n",
      "Sentiment    object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                                            Sentence Sentiment\n",
      "0  The GeoSolutions technology will leverage Bene...  positive\n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
      "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
      "-----------------------------------Dataset 4-----------------------------------\n",
      "Label        int64\n",
      "Ticker      object\n",
      "Headline    object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "   Label Ticker                                           Headline\n",
      "0      0    MMM  Employer who stole nearly $3M in wages from 15...\n",
      "1      1    MMM  Huge new Facebook data leak exposed intimate d...\n",
      "2      0    MMM  A campaign has accelerated to turn a disused r...\n",
      "-----------------------------------Dataset 5-----------------------------------\n",
      "Label        int64\n",
      "Ticker      object\n",
      "Headline    object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "   Label Ticker                                           Headline\n",
      "0      0      A  @TotesTravel : Airline shares tumble as New Yo...\n",
      "1      1      A  @TotesTravel : American United call off Hong K...\n",
      "2      0      A  @TotesTravel : U.S. airline stocks hit highest...\n",
      "-----------------------------------Dataset 6-----------------------------------\n",
      "Unnamed: 0        int64\n",
      "text             object\n",
      "timestamp        object\n",
      "source           object\n",
      "symbols          object\n",
      "company_names    object\n",
      "Sentiment         int64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "   Unnamed: 0                                               text  \\\n",
      "0           0  VIDEO: âI was in my office. I was minding my...   \n",
      "1           1  The price of lumber $LB_F is down 22% since hi...   \n",
      "2           2  Who says the American Dream is dead? https://t...   \n",
      "\n",
      "                        timestamp        source symbols      company_names  \\\n",
      "0  Wed Jul 18 21:33:26 +0000 2018  GoldmanSachs      GS  The Goldman Sachs   \n",
      "1  Wed Jul 18 22:22:47 +0000 2018    StockTwits       M             Macy's   \n",
      "2  Wed Jul 18 22:32:01 +0000 2018     TheStreet     AIG           American   \n",
      "\n",
      "   Sentiment  \n",
      "0          0  \n",
      "1          0  \n",
      "2         -1  \n"
     ]
    }
   ],
   "source": [
    "financial_data = [news, headlines, sentences, djia, nasdaq, tweets]\n",
    "\n",
    "# Observing the data types and the head of each original dataset\n",
    "for index, dataset in enumerate(financial_data):\n",
    "    print(('-' * 35) + 'Dataset ' + str(index + 1) + ('-' * 35))\n",
    "    print(dataset.dtypes)\n",
    "    print('\\n')\n",
    "    print(dataset.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the column indices by selecting only the relevant columns and dropping the rest\n",
    "news      = news[['sentiment', 'text']]\n",
    "sentences = sentences[['Sentiment', 'Sentence']]\n",
    "djia      = djia[['Label', 'Headline']]\n",
    "nasdaq    = nasdaq[['Label', 'Headline']]\n",
    "tweets    = tweets[['Sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data = [news, headlines, sentences, djia, nasdaq, tweets]\n",
    "\n",
    "for index, dataset in enumerate(financial_data):\n",
    "    # Standardizing the column names to 'label' and 'text'\n",
    "    dataset = dataset.rename(columns={dataset.columns[0]: 'label', dataset.columns[1]: 'text'})\n",
    "    # Converting the 'label' column from int64 type to string/object type\n",
    "    dataset['label'] = dataset['label'].astype(str)\n",
    "\n",
    "    # Normalizing the values in the 'label' column to follow the same numbering logic\n",
    "    if index == 0:\n",
    "        dataset['label'].replace({'1': '2', '2': '1'}, inplace=True)\n",
    "    elif index == 5:\n",
    "        dataset['label'].replace({'-1': '2'}, inplace=True)\n",
    "    elif index == 1 or index == 2:\n",
    "        dataset['label'].replace({'negative': '0', 'positive': '1', 'neutral': '2'}, inplace=True)\n",
    "    \n",
    "    # Mapping the values in the 'label' column to match the required input format of Amazon's BlazingText Classification Algorithm \n",
    "    dataset['label'] = dataset.label.map({\n",
    "        '0': '__label__negative__',\n",
    "        '1': '__label__positive__',\n",
    "        '2': '__label__neutral__'})\n",
    "    \n",
    "    financial_data[index] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------Dataset 1-----------------------------------\n",
      "label    object\n",
      "text     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                 label                                               text\n",
      "0  __label__negative__     Global COVID-19 death toll exceeds 4 million. \n",
      "1  __label__negative__  reports 67,208 new COVID-19 cases, 2,330 deaths. \n",
      "2  __label__negative__  China reports 23 new COVID-19 cases versus 19 ...\n",
      "-----------------------------------Dataset 2-----------------------------------\n",
      "label    object\n",
      "text     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                 label                                               text\n",
      "0   __label__neutral__  According to Gran , the company has no plans t...\n",
      "1   __label__neutral__  Technopolis plans to develop in stages an area...\n",
      "2  __label__negative__  The international electronic industry company ...\n",
      "-----------------------------------Dataset 3-----------------------------------\n",
      "label    object\n",
      "text     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                 label                                               text\n",
      "0  __label__positive__  The GeoSolutions technology will leverage Bene...\n",
      "1  __label__negative__  $ESI on lows, down $1.50 to $2.50 BK a real po...\n",
      "2  __label__positive__  For the last quarter of 2010 , Componenta 's n...\n",
      "-----------------------------------Dataset 4-----------------------------------\n",
      "label    object\n",
      "text     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                 label                                               text\n",
      "0  __label__negative__  Employer who stole nearly $3M in wages from 15...\n",
      "1  __label__positive__  Huge new Facebook data leak exposed intimate d...\n",
      "2  __label__negative__  A campaign has accelerated to turn a disused r...\n",
      "-----------------------------------Dataset 5-----------------------------------\n",
      "label    object\n",
      "text     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                 label                                               text\n",
      "0  __label__negative__  @TotesTravel : Airline shares tumble as New Yo...\n",
      "1  __label__positive__  @TotesTravel : American United call off Hong K...\n",
      "2  __label__negative__  @TotesTravel : U.S. airline stocks hit highest...\n",
      "-----------------------------------Dataset 6-----------------------------------\n",
      "label    object\n",
      "text     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "                 label                                               text\n",
      "0  __label__negative__  VIDEO: âI was in my office. I was minding my...\n",
      "1  __label__negative__  The price of lumber $LB_F is down 22% since hi...\n",
      "2   __label__neutral__  Who says the American Dream is dead? https://t...\n"
     ]
    }
   ],
   "source": [
    "# Observing the data types and the head of each modified dataset\n",
    "for index, dataset in enumerate(financial_data):\n",
    "    print(('-' * 35) + 'Dataset ' + str(index + 1) + ('-' * 35))\n",
    "    print(dataset.dtypes)\n",
    "    print('\\n')\n",
    "    print(dataset.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (54901, 2)\n",
      "Index(['label', 'text'], dtype='object')\n",
      "Labels: ['__label__negative__' '__label__neutral__' '__label__positive__']\n",
      "\n",
      "\n",
      "                 label                                               text\n",
      "0  __label__negative__     Global COVID-19 death toll exceeds 4 million. \n",
      "1  __label__negative__  reports 67,208 new COVID-19 cases, 2,330 deaths. \n",
      "2  __label__negative__  China reports 23 new COVID-19 cases versus 19 ...\n"
     ]
    }
   ],
   "source": [
    "# Concatenating all six datasets into a single dataset\n",
    "data = pd.concat(financial_data)\n",
    "\n",
    "# Observing the shape of the data, the column names, the label values, and the head of the data\n",
    "print('Data shape:', data.shape)\n",
    "print(data.columns)\n",
    "print('Labels:', data.label.unique())\n",
    "print('\\n')\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pre-Process the Data</h2>\n",
    "<p style=\"text-align: justify;\"> After compiling the dataset, we observe 54,901 lines of text along with their corresponding label. We now use the Natural Language Toolkit to tokenize each line. This particular tokenizer divides strings into space-separated substrings and requires the Punkt sentence tokenization models to be installed. Next, we check for and remove duplicate values from the dataset because this would skew the data otherwise. We partition the dataset into an 80% training set and a 20% validation set and save the corresponding datasets as a text file. Finally, we declare three variables that represent the location paths to the <em>Amazon S3 Bucket</em> for the input training and validation text files, as well as for the output files. We then proceed to upload the training and validation text files to the <em> S3 Bucket </em> by supplying the appropriate path. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and applying string manipulations to each line in the dataset \n",
    "data['text'] = data['text'].apply(nltk.word_tokenize)\n",
    "data['text'] = data.apply(lambda row: \" \".join(row['text']).lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 label                                               text\n",
      "0  __label__negative__     global covid-19 death toll exceeds 4 million .\n",
      "1  __label__negative__  reports 67,208 new covid-19 cases , 2,330 deat...\n",
      "2  __label__negative__  china reports 23 new covid-19 cases versus 19 ...\n",
      "\n",
      "\n",
      "                      label                                               text\n",
      "count                 54901                                              54901\n",
      "unique                    3                                              43394\n",
      "top     __label__negative__  rt @ morganlbrennan : with # helsinki2018 unde...\n",
      "freq                  28873                                                 69\n"
     ]
    }
   ],
   "source": [
    "# Observing the head of the data and describing the data\n",
    "print(data.head(3))\n",
    "print('\\n')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (43394, 2)\n",
      "\n",
      "\n",
      "                      label                                               text\n",
      "count                 43394                                              43394\n",
      "unique                    3                                              43394\n",
      "top     __label__negative__  why is @ att @ attcares not acknowledging majo...\n",
      "freq                  23884                                                  1\n"
     ]
    }
   ],
   "source": [
    "# Removing 11,507 duplicate lines in the dataset\n",
    "data = data.drop_duplicates(subset=['text'], keep='first')\n",
    "\n",
    "# Observing the shape of the data and describing the data\n",
    "print('Data shape:', data.shape)\n",
    "print('\\n')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (34715, 2)\n",
      "Validation shape: (8679, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Partitioning the data as follows:\n",
    "#   train:      80% of observations used to train the model\n",
    "#   validation: 20% of observartions used to test the model\n",
    "train, validation = train_test_split(data, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Displaying the shape of the training and validation partitioned data\n",
    "print('Training shape:', train.shape)\n",
    "print('Validation shape:', validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train file\n",
      "__label__negative__ $ 0.63 eps expected for nrg energy inc. $ nrg - https : //t.co/w4ukbmow0r\n",
      "__label__negative__ united nations closes sri lanka mission after protests | world news | the guardian\n",
      "__label__negative__ iraqi premier maliki removes key army chiefs -appoints own son to be supreme commander of the armed forces\n",
      "\n",
      "\n",
      "validation file\n",
      "__label__negative__ equities analysts set expectations for prudential financial incâs q2 2018 earnings $ pru https : //t.co/h7xltcjg3s\n",
      "__label__neutral__ intel chairman planning up to us $ 1b health-tech spac ipo\n",
      "__label__positive__ $ wfc wells fargo & amp ; company sec filing : form fwp https : //t.co/jazre8lctr\n"
     ]
    }
   ],
   "source": [
    "# Saving the training and validation datasets as .txt files\n",
    "np.savetxt('/tmp/training.txt', train.values, fmt='%s')\n",
    "np.savetxt('/tmp/validation.txt', validation.values, fmt='%s')\n",
    "\n",
    "# Displaying the head of the training and validation files \n",
    "!echo 'train file'\n",
    "!head -3 /tmp/training.txt\n",
    "!echo -e '\\n'\n",
    "!echo 'validation file'\n",
    "!head -3 /tmp/validation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables for the location paths of the input training and testing text files accordingly, as well as the output files\n",
    "prefix = 'financial-data'\n",
    "\n",
    "train_path = session.upload_data(path='/tmp/training.txt', bucket=bucket, key_prefix=prefix+'/input/train')\n",
    "val_path = session.upload_data(path='/tmp/validation.txt', bucket=bucket, key_prefix=prefix+'/input/validation')\n",
    "\n",
    "out_path = 's3://{}/{}/output/'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train the Algorithm</h2>\n",
    "<p style=\"text-align: justify;\"> We are now ready to use <em>Amazon Sagemakr's BlazingText Classification </em> algorithm. For this business problem, we will be using the supervised learning algorithm to solve the outlined label classification problem. This algorithm takes a text file as input and outputs a list of labels and their corresponding probability for each line of the inputted text file. More specifically, in terms of the given domain, the algorithm will output the three predefined labels (positive, negative, and neutral), along with a probability for each label. We configure the <em>Estimator</em> for the <em>BlazingText Classification </em>algorithm and specify the <em>mode</em> hyperparameter to <em>supervised</em>. Note that this is the only required hyperparameter and we explore the optional hyperparameters below. Afterwards, we define the training and testing data channels for the text files and set their <em>content_type</em> to \"<em>text/plain</em>\". </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve \n",
    "from sagemaker.estimator import Estimator \n",
    "\n",
    "role = get_execution_role()\n",
    "container = retrieve('blazingtext', region) \n",
    "\n",
    "# Initializing and configuring the Amazon Sagemaker BlazingText Classification Algorithm\n",
    "bt_estimator = Estimator(\n",
    "    container, \n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.large', \n",
    "    output_path=out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\"> We now tune the model by supplying a <em>learning_rate</em> of 0.05. The learning rate hyperparameter controls the speed at which the model learns, specifically, in response to the estimated error at the end of each line. Additionally, we set the number of <em>epochs</em> to 30 and the <em>patience</em> to five, which indicates the number of epochs to wait before applying early stopping when no progress is made on the validation set. Finally, the model training process is further equipped with <em>early_stopping</em> logic so as to not overtrain the model on the training dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the hyperparameters for the Amazon Sagemaker BlazingText Classification Algorithm\n",
    "bt_estimator.set_hyperparameters(\n",
    "    mode           = 'supervised', # REQUIRED\n",
    "    early_stopping = 'True',\n",
    "    epochs         = 30,\n",
    "    learning_rate  = 0.05,\n",
    "    patience       = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingInput \n",
    "\n",
    "# Defining the training and validation data channels for the text files accordingly\n",
    "train_data_channel = sagemaker.TrainingInput(train_path, content_type='text/plain')\n",
    "val_data_channel = sagemaker.TrainingInput(val_path, content_type='text/plain')\n",
    "\n",
    "channels = {\n",
    "    'train': train_data_channel, \n",
    "    'validation': val_data_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\"> Here, we launch the training job by supplying the data channels to the <em>fit()</em> API. Please note that this may take several minutes. Additionally, a series of messages will be outputted to the console that log the model training process and conclude by notifying that the training job is completed, along with the length of time required to train the model, in seconds. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-20 21:56:55 Starting - Starting the training job...\n",
      "2022-04-20 21:57:13 Starting - Preparing the instances for trainingProfilerReport-1650491815: InProgress\n",
      "......\n",
      "2022-04-20 21:58:24 Downloading - Downloading input data......\n",
      "2022-04-20 21:59:20 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/20/2022 21:59:22 WARNING 140096740992832] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/20/2022 21:59:22 WARNING 140096740992832] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/20/2022 21:59:22 INFO 140096740992832] nvidia-smi took: 0.02528977394104004 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/20/2022 21:59:22 INFO 140096740992832] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[04/20/2022 21:59:22 INFO 140096740992832] Processing /opt/ml/input/data/train/training.txt . File size: 4.7202348709106445 MB\u001b[0m\n",
      "\u001b[34m[04/20/2022 21:59:22 INFO 140096740992832] Processing /opt/ml/input/data/validation/validation.txt . File size: 1.1466341018676758 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  10510\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/validation.txt\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0485  Progress: 2.92%  Million Words/sec: 1.54 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0456  Progress: 8.85%  Million Words/sec: 2.25 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0431  Progress: 13.88%  Million Words/sec: 2.49 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.768983\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0404  Progress: 19.16%  Million Words/sec: 1.72 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.7782\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0379  Progress: 24.29%  Million Words/sec: 1.43 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.789492\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.793525\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0351  Progress: 29.77%  Million Words/sec: 1.14 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.792833\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.79629\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0324  Progress: 35.14%  Million Words/sec: 1.09 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 11\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.794562\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0298  Progress: 40.33%  Million Words/sec: 1.15 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 12\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.795944\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 13\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.795714\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0272  Progress: 45.68%  Million Words/sec: 1.18 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 14\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799055\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0245  Progress: 50.92%  Million Words/sec: 1.14 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 15\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.798133\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 16\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.796059\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0216  Progress: 56.77%  Million Words/sec: 1.19 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 17\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799286\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 18\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.79894\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0185  Progress: 62.99%  Million Words/sec: 1.14 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 19\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.79652\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 20\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.800092\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0158  Progress: 68.32%  Million Words/sec: 1.12 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 21\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.79917\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\n",
      "2022-04-20 21:59:56 Uploading - Uploading generated training model\u001b[34m##### Alpha: 0.0131  Progress: 73.83%  Million Words/sec: 1.14 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 22\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799862\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 23\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799631\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0101  Progress: 79.78%  Million Words/sec: 1.17 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 24\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.800784\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 25\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.79871\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0073  Progress: 85.40%  Million Words/sec: 1.14 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 26\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799862\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0046  Progress: 90.86%  Million Words/sec: 1.18 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 27\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799977\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 28\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.800438\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 4 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0019  Progress: 96.17%  Million Words/sec: 1.19 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 29\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799286\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 5 epochs.\u001b[0m\n",
      "\u001b[34mReached patience. Terminating training.\u001b[0m\n",
      "\u001b[34mBest epoch: 24\u001b[0m\n",
      "\u001b[34mBest validation accuracy: 0.800784\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 1.22 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 1.22\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 20.18\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9999\u001b[0m\n",
      "\u001b[34mNumber of train examples: 34715\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.8008\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 8679\u001b[0m\n",
      "\n",
      "2022-04-20 22:02:36 Completed - Training job completed\n",
      "ProfilerReport-1650491815: NoIssuesFound\n",
      "Training seconds: 247\n",
      "Billable seconds: 247\n"
     ]
    }
   ],
   "source": [
    "# Launching the training job by supplying the training and testing data channels to the fit() API\n",
    "bt_estimator.fit(inputs=channels)\n",
    "\n",
    "# This may take a few minutes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deploy the Model</h2>\n",
    "<p style=\"text-align: justify;\"> We are ready to deploy our model in <em>Amazon Sagemaker</em>. We configure and launch a new training job and set the <em>instance_type</em> to '<em>ml.t2.medium</em>', a smaller instance to save money. Remember to always delete the endpoint after testing the model to avoid unnecessary costs. Again, please note that this may take several minutes. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# Deploying the model\n",
    "bt_predictor = bt_estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.t2.medium')\n",
    "\n",
    "# This may take a few minutes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Perform Inferences</h2>\n",
    "<p style=\"text-align: justify;\"> Below we supply the model with a new .csv file. Note that this file, <em>input_text.csv</em> has been included in this environment. Similar to the dataset, we tokenize the .csv file to become space-separated strings and save it as a .txt file before supplying it to the model. It is important to note that the input .csv file must contain no headers and only a single sentence on each line. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autodesk construction cloud introduces new data sharing capabilities in the near future \n",
      "\n",
      "twist bio down 14 % on expanded agilent complaint over alleged theft of trade secrets \n",
      "\n",
      "sanoma will continue to focus on investing in digital media and on strengthening its market positions \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = pd.read_csv('input_text.csv', encoding='latin-1', names=['text'])\n",
    "input_text['text'] = input_text['text'].apply(nltk.word_tokenize)\n",
    "input_text['text'] = input_text.apply(lambda row: \" \".join(row['text']).lower(), axis=1)\n",
    "\n",
    "np.savetxt('input_text.txt', input_text.values, fmt='%s')\n",
    "\n",
    "with open('input_text.txt') as newfile:\n",
    "    sentences = newfile.read().splitlines()\n",
    "\n",
    "for line in range(3):\n",
    "    print(sentences[line], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: b'{\"label\": [\"__label__positive__\", \"__label__negative__\", \"__label__neutral__\"], \"prob\": [0.9600581526756287, 0.03699394315481186, 0.0029779470060020685]}' \n",
      "\n",
      "Sentence 2: b'{\"label\": [\"__label__negative__\", \"__label__neutral__\", \"__label__positive__\"], \"prob\": [0.9836263656616211, 0.012111024931073189, 0.004292502999305725]}' \n",
      "\n",
      "Sentence 3: b'{\"label\": [\"__label__neutral__\", \"__label__positive__\", \"__label__negative__\"], \"prob\": [0.7150107026100159, 0.1932600885629654, 0.09175929427146912]}, ' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "             \n",
    "payload = {'instances' : sentences, 'configuration': {'k': 3}}\n",
    "\n",
    "bt_predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "response = bt_predictor.predict(payload)\n",
    "\n",
    "print('Sentence 1:', response[1:154],   '\\n')\n",
    "print('Sentence 2:', response[156:309], '\\n')\n",
    "print('Sentence 3:', response[311:463], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tearing down the endpoint to avoid unnecessary costs\n",
    "bt_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Report Generation Based on Inferences</h2>\n",
    "<p style=\"text-align: justify;\"> The output of this model gives the various classification buckets and a probability score attached to each label, for each sentence inputted. It is arranged from highest probability to lowest probability. Taking the first sentence as an example, the sentence \"autodesk construction cloud introduces new data sharing capabilities in the near future\" gives an output of 96% positive, and a probability of roughly 3% for both negative and neutral. From this result, Division Capital could place a buy order instantly for Autodesk stock using an API and other relevant tools and profit from the positive sentiment surrounding the specific stock. That is, Division Capital can use this tool to efficiently scan through a large number of financial news headlines and generate probabilities of the headline being in each category. From that, the company can identify headlines with high probability of positive sentiment and execute profitable trades accordingly. </p>\n",
    "    \n",
    "<p style=\"text-align: justify;\"> As the output returns the probabilities of each sentiment, the business is able to make decisions with consisderations for its own risk appetite. For instance, they could have a rule to execute a buy order only if there sentiments are at least 70% positive, i.e. if the positive label is accompanied by a probability higher than 0.7. </p>\n",
    "    \n",
    "<p style=\"text-align: justify;\"> The business can further perform analyses on the results of the model to determine the chance of getting a false positive or a false negative. With such an information, they can exercise caution when relying on the model, or make adjustments to improve the accuracy of the model, possibly by feeding more data into the training set. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sources</h2>\n",
    "<p style=\"text-align: justify;\"> The following Pandas documentation was useful when manipulating, transforming, and preparing the dataset. </p>\n",
    "<p><a href=\"https://pandas.pydata.org/docs/ \"> https://pandas.pydata.org/docs/  </a></p>\n",
    "\n",
    "</br> \n",
    "\n",
    "<p style=\"text-align: justify;\"> The following Natural Language Toolkit documentation was useful when tokenizing and manipulating text from the dataset. </p>\n",
    "<p><a href=\"https://www.nltk.org/ \"> https://www.nltk.org/  </a></p>\n",
    "\n",
    "</br> \n",
    "\n",
    "<p style=\"text-align: justify;\"> The following Amazon Web Services documentation for Amazon Sagemaker's BlazingText Classification Algorithm was useful when initializing, configuring, and troubleshooting the Amazon Sagemaker environment. </p>\n",
    "<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\"> https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html </a></p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<p style=\"text-align: justify;\"> The following Amazon Web Services documentation for BlazingText Classification Hyperparameters was useful to better understand the various hyperparamaters. </p>\n",
    "<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html\"> https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html </a></p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<p style=\"text-align: justify;\"> Several of the code blocks relating to setting up the environment, configuring and launching the training job as well as deploying and testing the model were taken from the <em>BlazingText on Amazon Reviews - Classification.ipynb</em> file provided in class. </p>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
